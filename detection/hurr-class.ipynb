{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/global/common/cori/software/theano/0.8.2/lib/python2.7/site-packages/')\n",
    "import theano\n",
    "sys.path.insert(0,'/global/common/cori/software/lasagne/0.1/lib/python2.7/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Usage example employing Lasagne for digit recognition using the MNIST dataset.\n",
    "This example is deliberately structured as a long flat file, focusing on how\n",
    "to use Lasagne, instead of focusing on writing maximally modular and reusable\n",
    "code. It is used as the foundation for the introductory Lasagne tutorial:\n",
    "http://lasagne.readthedocs.org/en/latest/user/tutorial.html\n",
    "More in-depth examples and reproductions of paper results are maintained in\n",
    "a separate repository: https://github.com/Lasagne/Recipes\n",
    "\"\"\"\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "import lasagne.layers as lay\n",
    "import lasagne.objectives as obj\n",
    "import lasagne.nonlinearities as non\n",
    "import lasagne.updates as upd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5ls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6f1347141f23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mh5ls\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'h5ls' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: h5ls: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!h5ls /global/project/projectdirs/nervana/yunjie/dataset/localization/larger_hurricanes_loc.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ################## Download and prepare the MNIST dataset ##################\n",
    "# This is just some way of getting the MNIST dataset from an online location\n",
    "# and loading it into numpy arrays. It doesn't involve Lasagne at all.\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "#     path = '/global/project/projectdirs/nervana/yunjie/dataset/localization/larger_hurricanes_loc.h5'\n",
    "#     new_loc = os.path.join(os.environ['SCRATCH'], 'hurricane_detection')\n",
    "#     if not os.path.exists(new_loc):\n",
    "#         h5f = h5py.File(path)\n",
    "#         new_h5f = h5py.File(os.path.join(new_loc, 'hur.h5'))\n",
    "#         hurs = h5f.\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    \n",
    "#     if sys.version_info[0] == 2:\n",
    "#         from urllib import urlretrieve\n",
    "#     else:\n",
    "#         from urllib.request import urlretrieve\n",
    "\n",
    "#     def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "#         print(\"Downloading %s\" % filename)\n",
    "#         urlretrieve(source + filename, filename)\n",
    "\n",
    "#     # We then define functions for loading MNIST images and labels.\n",
    "#     # For convenience, they also download the requested files if needed.\n",
    "#     import gzip\n",
    "\n",
    "#     def load_mnist_images(filename):\n",
    "#         if not os.path.exists(filename):\n",
    "#             download(filename)\n",
    "#         # Read the inputs in Yann LeCun's binary format.\n",
    "#         with gzip.open(filename, 'rb') as f:\n",
    "#             data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "#         # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "#         # following the shape convention: (examples, channels, rows, columns)\n",
    "#         data = data.reshape(-1, 1, 28, 28)\n",
    "#         # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "#         # (Actually to range [0, 255/256], for compatibility to the version\n",
    "#         # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "#         return data / np.float32(256)\n",
    "\n",
    "#     def load_mnist_labels(filename):\n",
    "#         if not os.path.exists(filename):\n",
    "#             download(filename)\n",
    "#         # Read the labels in Yann LeCun's binary format.\n",
    "#         with gzip.open(filename, 'rb') as f:\n",
    "#             data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "#         # The labels are vectors of integers now, that's exactly what we want.\n",
    "#         return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None, depth=2):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28), input_var=input_var)\n",
    "    \n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.Conv2DLayer(network, num_filters=32, filter_size=(5,5),\n",
    "                                            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                            W=lasagne.init.HeNormal())\n",
    "        network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2,2))\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "                                lasagne.layers.dropout(network, p=0.5),\n",
    "                                num_units=256,\n",
    "                                nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(network, \n",
    "                                        num_units=10,\n",
    "                                        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0,len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "                \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class early_stop(object):\n",
    "    def __init__(self, patience=500):\n",
    "        self.patience = patience   # look as this many epochs regardless\n",
    "        self.patience_increase = 2  # wait this much longer when a new best is\n",
    "                                      # found\n",
    "        self.improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                      # considered significant\n",
    "        self.validation_frequency = self.patience // 2\n",
    "                                      # go through this many\n",
    "                                      # minibatche before checking the network\n",
    "                                      # on the validation set; in this case we\n",
    "                                      # check every epoch\n",
    "\n",
    "        self.best_validation_loss = np.inf\n",
    "\n",
    "    def keep_training(self, val_loss, epoch):\n",
    "        print epoch\n",
    "        print val_loss\n",
    "        print self.best_validation_loss\n",
    "        if val_loss < self.best_validation_loss:\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if val_loss < self.best_validation_loss *  \\\n",
    "                   self.improvement_threshold:\n",
    "                    self.patience = max(self.patience, epoch * self.patience_increase)\n",
    "\n",
    "                self.best_validation_loss = val_loss\n",
    "        if self.patience <= epoch:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_training(train_errs_or_accs, val_errs_or_accs, err_or_acc):\n",
    "        plt.figure(1 if err_or_acc == 'err' else 2)\n",
    "        plt.clf()\n",
    "        plt.title('Train/Val ' + err_or_acc)\n",
    "        plt.plot(train_errs_or_accs, label='train ' + err_or_acc)\n",
    "        plt.plot(val_errs_or_accs, label='val' + err_or_acc)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"loading data...\")\n",
    "#X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "datasets = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_network(depth=4,width=700, load=False):\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "\n",
    "    #network = build_mlp(input_var)\n",
    "    #network = build_custom_mlp(input_var, depth=depth, width=width)\n",
    "    network = build_cnn(input_var)\n",
    "    if load:\n",
    "        with np.load('model.npz') as f:\n",
    "            param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "            lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network, deterministic=False)\n",
    "    loss = obj.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lay.get_all_params(network, trainable=True)\n",
    "    updates = upd.nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                                target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                          dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    return train_fn, val_fn, network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasagne.init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n"
     ]
    }
   ],
   "source": [
    "train_fn, val_fn, network = build_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(network, train_fn, val_fn, datasets, num_epochs, save=False):\n",
    "    print(\"Starting training...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = datasets\n",
    "    train_errs = []\n",
    "    train_accs = []\n",
    "    val_errs = []\n",
    "    val_accs = []\n",
    "    #estop = early_stop(num_epochs)\n",
    "    epoch = 0\n",
    "#     while True:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            _, acc = val_fn(inputs, targets)\n",
    "            train_acc += acc\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        \n",
    "        train_accs.append(train_acc)\n",
    "        train_errs.append(train_err)\n",
    "        val_errs.append(val_err)\n",
    "        val_accs.append(val_acc)\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "        \n",
    "        \n",
    "        plot_training(train_errs,val_errs, 'err')\n",
    "        plot_training(train_accs,val_accs, 'acc')\n",
    "        \n",
    "#         if not estop.keep_training(val_err, epoch):\n",
    "#             break\n",
    "#         epoch += 1\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "\n",
    "    if save:\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "        np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "train(network, train_fn, val_fn,datasets,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
