{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from scripts.nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from notebooks/run_dir.ipynb\n",
      "importing Jupyter notebook from notebooks/helper_fxns.ipynb\n",
      "importing Jupyter notebook from notebooks/data_loader.ipynb\n",
      "importing Jupyter notebook from notebooks/train_val.ipynb\n",
      "importing Jupyter notebook from notebooks/build_network.ipynb\n",
      "importing Jupyter notebook from notebooks/print_n_plot.ipynb\n"
     ]
    }
   ],
   "source": [
    "'''before we import theano anywhere else we want to make sure we specify \n",
    "a unique directory for compiling, so we dont get into a locking issue\n",
    "if we run multiple hur_mains at once on a global file system. Haven't truly implementedthis yet '''\n",
    "from notebooks.run_dir import create_run_dir\n",
    "from notebooks.helper_fxns import dump_hyperparams\n",
    "from notebooks.data_loader import load_data, load_precomputed_data\n",
    "from notebooks.train_val import train\n",
    "from notebooks.print_n_plot import plot_ims_with_boxes\n",
    "from notebooks.build_network import build_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if inside a notebook, then get rid of weird notebook arguments, so that arg parsing still works\n",
    "if any([\"jupyter\" in arg for arg in sys.argv]):\n",
    "    sys.argv=sys.argv[:1]\n",
    "    \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-e', '--epochs', type=int, default=10000,\n",
    "    help='number of epochs for training')\n",
    "\n",
    "parser.add_argument('-l', '--learn_rate', default=0.0001, type=float,\n",
    "    help='the learning rate for the network')\n",
    "\n",
    "parser.add_argument('-n', '--num_ims', default=30, type=int,\n",
    "    help='number of total images')\n",
    "\n",
    "parser.add_argument('-f', '--num_filters', default=128, type=int,\n",
    "    help='number of filters in each conv layer')\n",
    "\n",
    "parser.add_argument( '--fc', default=512, type=int,\n",
    "    help='number of fully connected units')\n",
    "\n",
    "parser.add_argument('--coord_penalty', default=5, type=int,\n",
    "    help='penalty for guessing coordinates wrong')\n",
    "\n",
    "parser.add_argument('--size_penalty', default=5, type=int,\n",
    "    help='penalty for guessing height or width wrong')\n",
    "\n",
    "parser.add_argument('--nonobj_penalty', default=0.5, type=float,\n",
    "    help='penalty for guessing an object where one isnt')\n",
    "\n",
    "parser.add_argument('-c','--num_extra_conv', default=0, type=int,\n",
    "    help='conv layers to add on to each conv layer before max pooling')\n",
    "\n",
    "parser.add_argument('--num_convpool', default=4, type=int,\n",
    "    help='number of conv layer-pool layer pairs')\n",
    "\n",
    "parser.add_argument('--momentum', default=0.9, type=float,\n",
    "    help='momentum')\n",
    "\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making run num file....\n",
      "/global/u1/r/racah/projects/hur-detect/results/run1\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "Unable to create file (Unable to open file: name = '/global/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/theano_data/theano_hur_train.h5', errno = 2, error message = 'no such file or directory', flags = 15, o_flags = c2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a614eb8ec99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset = load_precomputed_data(paths=[\"/global/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/theano_data/theano_hur_train.h5\",\n\u001b[1;32m      6\u001b[0m                                 \"/global/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/theano_data/theano_hur_val.h5\" ],\n\u001b[0;32m----> 7\u001b[0;31m                                 out_of_core=True)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# dataset = load_data(num_ims=args.num_ims,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#                     path='/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/hur_train_val.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/u1/r/racah/projects/hur-detect/notebooks/data_loader.ipynb\u001b[0m in \u001b[0;36mload_precomputed_data\u001b[0;34m(paths, out_of_core)\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDONLY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid mode; must be one of r, r+, w, w-, x, a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/work/h5py/_objects.c:2696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/work/h5py/_objects.c:2654)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create (/home/ilan/minonda/conda-bld/work/h5py/h5f.c:2109)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Unable to create file (Unable to open file: name = '/global/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/theano_data/theano_hur_train.h5', errno = 2, error message = 'no such file or directory', flags = 15, o_flags = c2)"
     ]
    }
   ],
   "source": [
    "\n",
    "run_dir = create_run_dir()\n",
    "print run_dir\n",
    "\n",
    "dataset = load_precomputed_data(paths=[\"/global/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/theano_data/theano_hur_train.h5\",\n",
    "                                \"/global/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/theano_data/theano_hur_val.h5\" ],\n",
    "                                out_of_core=True)\n",
    "# dataset = load_data(num_ims=args.num_ims,\n",
    "#                     path='/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/hur_train_val.h5')\n",
    "\n",
    "'''size of ground truth grid'''\n",
    "grid_size = dataset[1].shape[1]\n",
    "\n",
    "'''set params'''\n",
    "network_kwargs = {'learning_rate': args.learn_rate, \n",
    "                  'dropout_p': 0, \n",
    "                  'weight_decay': 0, \n",
    "                  'num_filters': args.num_filters, \n",
    "                  'num_fc_units': args.fc, \n",
    "                  'num_convpool': args.num_convpool,\n",
    "                  'num_extra_conv': args.num_extra_conv,\n",
    "                  'momentum': args.momentum,\n",
    "                  'coord_penalty': args.coord_penalty,\n",
    "                  'nonobj_penalty': args.nonobj_penalty,\n",
    "                   }\n",
    "\n",
    "\n",
    "'''get network and train_fns'''\n",
    "train_fn, val_fn, box_fn, network, hyperparams = build_network(**network_kwargs)\n",
    "\n",
    "hyperparams.update({'num_ims': args.num_ims, 'tr_size': dataset[0].shape[0]})\n",
    "'''save hyperparams'''\n",
    "dump_hyperparams(hyperparams, path=run_dir)\n",
    "\n",
    "'''train'''\n",
    "train(dataset, network=network, fns=(train_fn, val_fn, box_fn),num_ims=args.num_ims, save_weights=True, num_epochs=args.epochs, save_path=run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
