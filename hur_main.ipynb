{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from notebooks/run_dir.ipynb\n",
      "importing Jupyter notebook from notebooks/helper_fxns.ipynb\n",
      "importing Jupyter notebook from notebooks/data_loader.ipynb\n",
      "importing Jupyter notebook from notebooks/train_val.ipynb\n",
      "importing Jupyter notebook from notebooks/print_n_plot.ipynb\n",
      "importing Jupyter notebook from notebooks/build_network.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from scripts.nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "\n",
    "'''before we import theano anywhere else we want to make sure we specify \n",
    "a unique directory for compiling, so we dont get into a locking issue\n",
    "if we run multiple hur_mains at once on a global file system. Haven't truly implementedthis yet '''\n",
    "from notebooks.run_dir import create_run_dir\n",
    "run_dir = create_run_dir()\n",
    "from notebooks.helper_fxns import dump_hyperparams\n",
    "from notebooks.data_loader import load_classification_dataset, load_detection_dataset\n",
    "from notebooks.train_val import train\n",
    "from notebooks.print_n_plot import plot_ims_with_boxes\n",
    "from notebooks.build_network import build_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data...\n",
      "40\n",
      "running on non pretrained classif network!\n",
      "Building model and compiling functions...\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 3 took 7.011s\n",
      "\ttraining los:\t\t7.0479\n",
      "\ttraining acc:\t\t0.0000 %\n",
      "  validation loss:\t\t8.479027\n",
      "  validation accuracy:\t\t0.00 %\n",
      "Epoch 2 of 3 took 6.212s\n",
      "\ttraining los:\t\t6.9420\n",
      "\ttraining acc:\t\t0.0000 %\n",
      "  validation loss:\t\t8.437138\n",
      "  validation accuracy:\t\t0.00 %\n",
      "Epoch 3 of 3 took 6.987s\n",
      "\ttraining los:\t\t6.8458\n",
      "\ttraining acc:\t\t0.0000 %\n",
      "  validation loss:\t\t8.381370\n",
      "  validation accuracy:\t\t0.00 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "learning_rate = 0.0001\n",
    "num_ims = 40\n",
    "num_filters = 128\n",
    "num_fc_units = 128\n",
    "lc =5 \n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-e', '--epochs', type=int, default=10,\n",
    "#     help='number of epochs for training')\n",
    "\n",
    "# parser.add_argument('-l', '--learn_rate', default=0.0001, type=float,\n",
    "#     help='the learning rate for the network')\n",
    "\n",
    "# parser.add_argument('-n', '--num_ims', default=40, type=int,\n",
    "#     help='number of total images')\n",
    "\n",
    "# parser.add_argument('-f', '--num_filters', default=512, type=int,\n",
    "#     help='number of filters in each conv layer')\n",
    "\n",
    "# parser.add_argument('-c', '--num_fc_units', default=512, type=int,\n",
    "#     help='number of fully connected units')\n",
    "\n",
    "# parser.add_argument('--coord_loss', default=5, type=int,\n",
    "#     help='penalty for guessing coordinates or height wrong')\n",
    "# args = parser.parse_args()\n",
    "# epochs = args.epochs\n",
    "# learning_rate = args.learn_rate\n",
    "# num_ims = args.num_ims\n",
    "# num_filters = args.num_filters\n",
    "# lc = args.coord_loss\n",
    "\n",
    "\n",
    "run_dir = create_run_dir()\n",
    "\n",
    "dataset = load_detection_dataset(num_ims=num_ims)\n",
    "\n",
    "'''size of ground truth grid'''\n",
    "grid_size = dataset[1].shape[1]\n",
    "\n",
    "'''set params'''\n",
    "network_kwargs = {'learning_rate': learning_rate, 'dropout_p': 0, 'weight_decay': 0, \n",
    "                  'num_filters': num_filters, 'num_fc_units': num_fc_units}\n",
    "detec_network_kwargs = {'grid_size': grid_size, 'nclass': 1, 'n_boxes':1, 'lc': lc}\n",
    "\n",
    "\n",
    "'''get network and train_fns'''\n",
    "train_fn, val_fn, box_fn, network, hyperparams = build_network(mode='detection', \n",
    "                                                               network_kwargs=network_kwargs, \n",
    "                                                               detec_specific_kwargs=detec_network_kwargs)\n",
    "\n",
    "hyperparams.update({'num_ims': num_ims, 'tr_size': dataset[0].shape[0]})\n",
    "'''save hyperparams'''\n",
    "dump_hyperparams(hyperparams, path=run_dir)\n",
    "\n",
    "'''train'''\n",
    "train(dataset, network=network, fns=(train_fn, val_fn, box_fn), num_epochs=epochs, save_path=run_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
