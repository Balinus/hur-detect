{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from helper_fxns.ipynb\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "from lasagne.layers import Conv2DLayer\n",
    "from lasagne.layers import MaxPool2DLayer\n",
    "from lasagne.layers import dropout\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.nonlinearities import rectify\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import sys\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "#enable importing of notebooks\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "import inspect\n",
    "from lasagne.nonlinearities import *\n",
    "from lasagne.objectives import *\n",
    "from helper_fxns import get_detec_loss, get_boxes_ap\n",
    "#if __name__ == \"__main__\":\n",
    "    #from data_loader import load_classification_dataset, load_detection_dataset\n",
    "\n",
    "def get_hyperparams(frame):\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    #return dict(zip(args,values))\n",
    "    #del values['frame']\n",
    "    return values\n",
    "\n",
    "def build_network(    input_shape=(1,16,768,1152),\n",
    "                      num_classes = 4,\n",
    "                      filter_dim=3,\n",
    "                      num_convpool=4,\n",
    "                      scale_factor=64,\n",
    "                      num_filters=128,\n",
    "                      num_layers = 6,\n",
    "                      num_fc_units=1024,\n",
    "                      num_extra_conv=2, \n",
    "                      nonlinearity=lasagne.nonlinearities.LeakyRectify(0.1),\n",
    "                      w_init=lasagne.init.HeUniform(),\n",
    "                      dropout_p=0.5,      \n",
    "                      learning_rate = 0.00001,\n",
    "                      weight_decay = 0.0005,\n",
    "                      momentum = 0.9,\n",
    "                      delta=0.00001,\n",
    "                      coord_penalty = 5,\n",
    "                      size_penalty = 5,\n",
    "                      nonobj_penalty = 0.5,\n",
    "                      n_boxes=1,\n",
    "                      nclass=1,\n",
    "                      grid_size=6,\n",
    "                      load=False,\n",
    "                      load_path=None):\n",
    "    \n",
    "    '''Takes a pretrained classification net and adds a few convolutional layers on top of it\n",
    "    and defines a detection loss function'''\n",
    "    '''Args:\n",
    "                      \n",
    "                      num_convpool: number of conv layer-pool layer pairs\n",
    "                      delta: smoothing constant to loss function (ie sqrt(x + delta)) \n",
    "                            -> if x is 0 gradient is undefined\n",
    "                      num_filters\n",
    "                      num_fc_units\n",
    "                      num_extra_conv: conv layers to add on to each conv layer before max pooling\n",
    "                      nonlinearity: which nonlinearity to use throughout\n",
    "                      n_boxes: how many boxes should be predicted at each grid point,\n",
    "                      nclass: how many classes are we predicting,\n",
    "                      grid_size: size of the grid that encodes various \n",
    "                                locations of image (ie in the YOLO paper they use 7x7 grid)\n",
    "                      w_init: weight intitialization\n",
    "                      dropout_p: prob of dropping unit\n",
    "                      coord_penalty : penalty in YOLO loss function for getting coordinates wrong\n",
    "                      nonobj_penalty: penalty in YOLO loss for guessing object when there isn't one\n",
    "                      learning_rate\n",
    "                      weight_decay\n",
    "                      momentum\n",
    "                      load: whether to load weights or not\n",
    "                      load_path: path for loading weights'''\n",
    "\n",
    "    #get all key,value args from function\n",
    "    hyperparams = get_hyperparams(inspect.currentframe())\n",
    "    \n",
    "    input_var = T.tensor4('input_var')\n",
    "    target_var = T.tensor4('target_var') #is of shape (grid_size, grid_size,(n_boxes* 5 + nclass)\n",
    "    \n",
    "    print \"Building model and compiling functions...\" \n",
    "    \n",
    "    #make layers\n",
    "    network = build_layers(input_var, **hyperparams)\n",
    "    \n",
    "    #load in any pretrained weights\n",
    "    if load:\n",
    "        network = load_weights(load_path, network)\n",
    "    \n",
    "    #compile theano functions\n",
    "    train_fn, val_fn,pred_fn, ap_box_fn = make_fns(network,input_var, target_var, coord_penalty, size_penalty, nonobj_penalty,\n",
    "                                        learning_rate, momentum, weight_decay, delta,scale_factor)\n",
    "    \n",
    "    #box_fn\n",
    "    return train_fn, val_fn,pred_fn, ap_box_fn, network, hyperparams\n",
    "\n",
    "def build_layers(input_var, **nk):\n",
    "    '''nk: network_kwargs'''\n",
    "    '''conv, extra_convs, pool multiple times then fc with dropout, fc with dropout and softmax then reshape'''\n",
    "    \n",
    "    '''total number of conv layers is num_convpool * (1 + num_extra_conv)'''\n",
    "    \n",
    "    filter_dim = nk['filter_dim']\n",
    "    base_num_filters = nk['num_filters']\n",
    "    num_layers = nk['num_layers']\n",
    "    num_filters = base_num_filters\n",
    "    \n",
    "    network = lasagne.layers.InputLayer(shape=nk['input_shape'], input_var=input_var)\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        network = Conv2DLayer(batch_norm(network), \n",
    "                              num_filters=num_filters, \n",
    "                              filter_size=nk['filter_dim'], \n",
    "                              pad=nk['filter_dim'] / 2, stride=2, W=nk['w_init'], nonlinearity=nk['nonlinearity'])\n",
    "        num_filters *= 2\n",
    "\n",
    "    \n",
    "    coord_net = Conv2DLayer(batch_norm(network), num_filters=5, filter_size=1,W=nk['w_init'], nonlinearity=rectify)\n",
    "    \n",
    "    class_net = Conv2DLayer(batch_norm(network), num_filters=nk['num_classes'], filter_size=1,W=nk['w_init'], nonlinearity=sigmoid)\n",
    "\n",
    "    network = ConcatLayer([coord_net, class_net])\n",
    "    \n",
    "    return network\n",
    "        \n",
    "\n",
    "def load_weights(file_path, network):\n",
    "    '''grabs weights from an npz file'''\n",
    "    with np.load(file_path) as f:\n",
    "        param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "        lasagne.layers.set_all_param_values(network, param_values)\n",
    "    return network\n",
    "    \n",
    "\n",
    "def make_fns(network,input_var, det_target_var, lcxy, lchw, ln, learning_rate, momentum, weight_decay, delta,scale_factor):\n",
    "    '''Compiles theano train, test, box_fns'''\n",
    "    #deterministic determines whether to use dropout or not in forward pass\n",
    "    #transpose output to match what loss expects\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    prediction = lasagne.layers.get_output(network, deterministic=False)\n",
    "    \n",
    "    \n",
    "    def make_loss(pred):\n",
    "        loss = get_detec_loss(pred, det_target_var, lcxy, lchw, ln, delta)\n",
    "        weightsl2 = lasagne.regularization.regularize_network_params(network, lasagne.regularization.l2)\n",
    "        loss += weight_decay * weightsl2\n",
    "        return loss.mean()\n",
    "    \n",
    "    def make_train_fn():\n",
    "        '''takes as input the input, target vars and ouputs a loss'''\n",
    "        \n",
    "        loss=  make_loss(prediction)\n",
    "        params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "        updates = lasagne.updates.adam(loss, params,learning_rate=learning_rate)\n",
    "        train_fn = theano.function([input_var, det_target_var], loss, updates=updates)\n",
    "        return train_fn\n",
    "        \n",
    "    \n",
    "    def make_test_or_val_fn():\n",
    "        '''takes as input the input, target vars and ouputs a non-dropout loss and an accuracy (intersection over union)'''\n",
    "        test_loss = make_loss(test_prediction)\n",
    "        val_fn = theano.function([input_var, det_target_var], test_loss)\n",
    "        return val_fn\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def make_pred_fn():\n",
    "        '''takes as input the input, target vars and outputs the predicted grid'''\n",
    "        pred_fn = theano.function([input_var], test_prediction)\n",
    "        return pred_fn\n",
    "        \n",
    "    def make_ap_box_fn():\n",
    "        '''takes as input the input, target vars and outputs the predicted and the ground truth boxes)'''\n",
    "        pred_fn = make_pred_fn()\n",
    "        def ap_box_fn(inp,gt,conf_thresh=0.7,iou_thresh=0.5):\n",
    "            pred = pred_fn(inp)\n",
    "            ap, pred_boxes, gt_boxes = get_boxes_ap(pred,gt, conf_thresh,iou_thresh)\n",
    "            return ap, pred_boxes, gt_boxes\n",
    "    \n",
    "        return ap_box_fn\n",
    "    \n",
    "    train_fn = make_train_fn()\n",
    "    test_or_val_fn = make_test_or_val_fn()\n",
    "    ap_box_fn = make_ap_box_fn()\n",
    "    pred_fn = make_pred_fn()\n",
    "    \n",
    "    return train_fn, test_or_val_fn,pred_fn, ap_box_fn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_fn, val_fn,pred_fn, ap_box_fn, network, hyperparams = build_network(num_filters=2)\n",
    "\n",
    "    from netcdf_loader import bbox_iterator\n",
    "\n",
    "    for e in range(10):\n",
    "        for x,y in bbox_iterator(years=[1979], days=3,data_dir=\"/storeSSD/eracah/data/netcdf_ims/\", metadata_dir=\"/storeSSD/eracah/data/metadata/\"):\n",
    "            x = np.squeeze(x,axis=2)\n",
    "            y = np.squeeze(y,axis=1)\n",
    "            #pred = pred_fn(x)\n",
    "            #print pred\n",
    "            #print pred.shape\n",
    "            loss = train_fn(x,y)\n",
    "            print loss\n",
    "            #vloss, acc = val_fn(x,y)\n",
    "            ap, pred_box, gt_box =  ap_box_fn(x,y, conf_thresh=0.5,iou_thresh=0.1)\n",
    "            print ap\n",
    "    #         print loss\n",
    "    #         print vloss\n",
    "    #         print acc\n",
    "            #print pred_box\n",
    "            #print gt_box\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
