{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from util.ipynb\n",
      "importing Jupyter notebook from ground_truth_maker.ipynb\n",
      "importing Jupyter notebook from label_loader.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import MFDataset\n",
    "from os import listdir, system\n",
    "from os.path import isfile, join, isdir\n",
    "import numpy as np\n",
    "import imp\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import inspect\n",
    "import copy\n",
    "from util import get_camfiles, normalize,convert_nc_data_to_tensor\n",
    "from ground_truth_maker import make_yolo_masks_for_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BBoxIterator(object):\n",
    "    def __init__(self,kwargs,mode=\"tr\"):\n",
    "        self.kwargs = kwargs\n",
    "        self.mode = mode\n",
    "        #frame = inspect.currentframe()\n",
    "        self.seed = 7\n",
    "        self.camfiles = get_camfiles(self.kwargs[\"data_dir\"], self.kwargs[mode+ \"_years\"])[:self.kwargs[\"num_\" + mode + \"_days\"]]\n",
    "        self.file_ind = 0\n",
    "        self.chunk_ind =0 \n",
    "        self.data_chunk=[]\n",
    "        self.num_events = len(self.camfiles) * 4\n",
    "        self.events_open = 4 * self.kwargs[\"max_files_open\"]\n",
    "       \n",
    "    \n",
    "    def iterate_chunks(self,batch_size=128):\n",
    "        chunk_index = 0\n",
    "        events_read =0 \n",
    "        data_chunk = self.get_next_chunk()\n",
    "        while events_read < self.num_events:\n",
    "            if chunk_index + batch_size > len(data_chunk):\n",
    "                data_chunk = self.finish_out_chunk_and_get_as_many_more_as_needed(data_chunk, \n",
    "                                                                                  chunk_index, \n",
    "                                                                                  batch_size)\n",
    "                #back to 0 b/c we have a brand new chunk\n",
    "                chunk_index = 0\n",
    "            \n",
    "            if events_read + batch_size > self.num_events:\n",
    "                sm_batch_size = self.num_events - events_read\n",
    "                excerpt = slice(chunk_index, chunk_index + sm_batch_size)\n",
    "            \n",
    "            else:\n",
    "\n",
    "                #otherwise just read an excerpt from the current chunk\n",
    "                excerpt = slice(chunk_index, chunk_index + batch_size)\n",
    "            \n",
    "            \n",
    "            chunk_index += batch_size\n",
    "            events_read += batch_size\n",
    "            \n",
    "            yield data_chunk[excerpt]\n",
    "            \n",
    "     \n",
    "    def finish_out_chunk_and_get_as_many_more_as_needed(self,data_chunk,ix, batch_size):\n",
    "        tmp = data_chunk[ix:]\n",
    "        data_chunk = self.get_chunks_until_at_capacity(batch_size - tmp.shape[0])\n",
    "        data_chunk = np.vstack((tmp,data_chunk))\n",
    "        return data_chunk\n",
    "        \n",
    "    def get_chunks_until_at_capacity(self,batch_size):\n",
    "        tmp = self.get_next_chunk()\n",
    "        while tmp.shape[0] < batch_size:\n",
    "            data_chunk = self.get_next_chunk()\n",
    "            tmp = np.vstack((tmp,data_chunk))\n",
    "        return tmp\n",
    "\n",
    "    def get_next_chunk(self):\n",
    "        mfo = kwargs[\"max_files_open\"]\n",
    "        \n",
    "        #if we are starting back up again shuffle everything\n",
    "        if self.file_ind < mfo:\n",
    "            if self.kwargs[\"shuffle\"]:\n",
    "                self.camfiles = self.camfiles.shuffle()\n",
    "        \n",
    "        #get next chunk of files\n",
    "        filenames = self.camfiles[self.file_ind: mfo + self.file_ind]\n",
    "        \n",
    "        #increment index to start with (modulo for circular effect)\n",
    "        self.file_ind = (self.file_ind + kwargs[\"max_files_open\"] ) % len(self.camfiles)\n",
    "        \n",
    "        return self._get_next_chunk(filenames)\n",
    "        \n",
    "        \n",
    "    def _get_next_chunk(self,filenames):\n",
    "        \n",
    "        data_chunk = self.grab_data_chunk(filenames)\n",
    "        \n",
    "        #self.label_chunk = self.grab_label_chunk(filenames)\n",
    "        return data_chunk\n",
    "        \n",
    "    \n",
    "    def grab_data_chunk(self, filenames):\n",
    "        \"\"\"grabs input data (converts filenames to numpy tensors)\n",
    "        returns len(filenames)*4, 16, 768,1152 array\"\"\"\n",
    "        \n",
    "        filenames = [join(self.kwargs[\"data_dir\"],f) for f in filenames]\n",
    "\n",
    "\n",
    "        dataset=MFDataset(filenames)\n",
    "        \n",
    "        tensor = convert_nc_data_to_tensor(dataset,self.kwargs)\n",
    " \n",
    "        return tensor\n",
    "        #if 3D -> convert to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 16, 768, 1152)\n",
      "(5, 16, 768, 1152)\n",
      "(2, 16, 768, 1152)\n",
      "3.75681400299\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sys.path.insert(0,\"/home/evan/hur-detect/scripts/\")\n",
    "    from configs import *\n",
    "    kwargs = process_kwargs()\n",
    "    kwargs[\"max_files_open\"] = 1\n",
    "    kwargs['num_val_days'] = 3\n",
    "    t = time.time()\n",
    "    for x in BBoxIterator(kwargs,mode=\"val\").iterate_chunks(batch_size=5):\n",
    "        print x.shape\n",
    "    print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
