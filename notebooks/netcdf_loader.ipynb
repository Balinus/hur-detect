{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from os import listdir, system\n",
    "from os.path import isfile, join, isdir\n",
    "import re\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "import imp\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"..\")\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 20\n",
    "import pdb\n",
    "import itertools\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import inspect\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you view a map:\n",
    " * longitude: horizontal\n",
    " * latitude: vertical\n",
    " \n",
    "Defined Here:\n",
    " * longitude (horiz): y\n",
    " * latitude (vertical): x\n",
    " \n",
    "Array-wise:\n",
    " * dim1(x) : vertical\n",
    " * dim2(y) : horizontal\n",
    " \n",
    "So:\n",
    " * dim1 of array is latitude (thus x)\n",
    " * dim2 is longitude (thus y)\n",
    "\n",
    "So if we define something as xmin,xmax,ymin,ymax here:\n",
    " * filling in that box in the array is:\n",
    "    * arr[xmin:xmax, ymin:ymax] = 0\n",
    "    \n",
    "    \n",
    "* and the array is 768,1152 ?\n",
    "\n",
    "\n",
    "\n",
    "#LABEL NUMBERS\n",
    "* Tropical Depression is 1\n",
    "* Hurricane is 2\n",
    "* ETC is 3\n",
    "* AR is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_grid(bbox, grid, xdim, ydim, scale_factor,num_classes, caffe_format=False):\n",
    "    cls = int(bbox[4])\n",
    "    x,y = bbox[0] / scale_factor, bbox[1] / scale_factor\n",
    "    xo,yo = (bbox[0] % scale_factor) / float(scale_factor), (bbox[1] % scale_factor) / float(scale_factor)\n",
    "    w,h = bbox[2] / scale_factor / (xdim / scale_factor), bbox[3] / scale_factor/ (ydim / scale_factor)\n",
    "    \n",
    "    depth = 5 + num_classes\n",
    "    if caffe_format:\n",
    "        l_box = grid[:depth,x,y]\n",
    "    else:\n",
    "        l_box = grid[int(x),int(y),:depth]\n",
    "    lbl = num_classes*[0]\n",
    "    lbl[cls-1] = 1\n",
    "    \n",
    "    real_box = [xo,yo,w,h,1.]\n",
    "    real_box.extend(lbl)\n",
    "    \n",
    "    print l_box\n",
    "    print real_box\n",
    "    assert np.allclose(l_box, real_box), \"Tests Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The etc files that use even time steps for labels:\n",
    "#1979, 1980, 1982, 1983, 1984, 1985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_timestamp(filename):\n",
    "    rpyear = re.compile(r\"(\\.h2\\.)(.*?)(-)\")\n",
    "    rpdaymonth = re.compile(r\"(-)(.*?)(\\d{5}\\.)\")\n",
    "    year=int(rpyear.search(filename).groups()[1])\n",
    "    tmp=rpdaymonth.search(filename).groups()[1].split('-')\n",
    "    month=int(tmp[0])\n",
    "    day=int(tmp[1])\n",
    "    return dt.date(year,month,day)\n",
    "\n",
    "def convert_bbox_minmax_to_cent_xywh(bboxes):\n",
    "    #current bbox set up is xmin,ymin,xmax,ymax\n",
    "    xmin, xmax,ymin,  ymax = [ bboxes[:,:,i] for i in range(4) ]\n",
    "    \n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "\n",
    "    x_c = xmin + w / 2.\n",
    "    y_c = ymin + h / 2.\n",
    "    \n",
    "    \n",
    "    bboxes[:,:,0] = x_c\n",
    "    bboxes[:,:,1] = y_c\n",
    "    bboxes[:,:,2] = w # w\n",
    "    bboxes[:,:,3] = h #h\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_camfiles(data_dir, years):\n",
    "    lsdir=listdir(data_dir)\n",
    "    rpfile = re.compile(r\"^cam5_.*\\.nc$\")\n",
    "    camfiles = [f for f in lsdir if rpfile.match(f)]\n",
    "    camfiles = [c for c in camfiles if get_timestamp(c).year in years]\n",
    "    camfiles.sort()\n",
    "    return camfiles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(arr,min_=None, max_=None, axis=(0,2,3)):\n",
    "        if min_ is None or max_ is None:\n",
    "            min_ = arr.min(axis=(0,2,3), keepdims=True)\n",
    "\n",
    "            max_ = arr.max(axis=(0,2,3), keepdims=True)\n",
    "\n",
    "        midrange = (max_ + min_) / 2.\n",
    "\n",
    "        range_ = (max_ - min_) / 2.\n",
    "        \n",
    "        arr -= midrange\n",
    "\n",
    "        arr /= (range_)\n",
    "        return arr, min_, max_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BBoxIterator(object):\n",
    "    def __init__(self,\n",
    "                 years=[1979],\n",
    "                 days=2,\n",
    "                 batch_size = 1,\n",
    "                 data_dir=\"/storeSSD/eracah/data/netcdf_ims/\", \n",
    "                 metadata_dir=\"/storeSSD/eracah/data/metadata/\",\n",
    "                 shuffle=False, \n",
    "                 num_classes=4, \n",
    "                 labels_only=True, \n",
    "                 time_chunks_per_example=1,\n",
    "                 no_labels_only=False,\n",
    "                 time_stride=None, \n",
    "                 scale_factor=64., seed =5, box_sizes=[(64,64)]):\n",
    "        \n",
    "        frame = inspect.currentframe()\n",
    "        self.set_data_members(frame)\n",
    "        self.variables = [u'PRECT',u'PS',u'PSL',\n",
    "                     u'QREFHT',\n",
    "                     u'T200',\n",
    "                     u'T500',\n",
    "                     u'TMQ',\n",
    "                     u'TREFHT',\n",
    "                     u'TS',\n",
    "                     u'U850',\n",
    "                     u'UBOT',\n",
    "                     u'V850',\n",
    "                     u'VBOT',\n",
    "                     u'Z1000',\n",
    "                     u'Z200',\n",
    "                     u'ZBOT'] \n",
    "        self.time_steps_per_day = 8\n",
    "        self.xdim = 768\n",
    "        self.ydim = 1152\n",
    "        self.seed = seed\n",
    "        self.camfiles = get_camfiles(self.data_dir, self.years)[:self.days]\n",
    "\n",
    "            \n",
    "    def set_data_members(self, frame):\n",
    "        args, _, _, values = inspect.getargvalues(frame)\n",
    "        del values['self']\n",
    "        del values['frame']\n",
    "        for k,v in values.iteritems():\n",
    "            setattr(self,k,v)\n",
    "            \n",
    "    def iterate(self):\n",
    "        for x,y in self.data_iterator():\n",
    "#             print x.shape\n",
    "#             print y.shape\n",
    "            x, y = np.swapaxes(x, 1, 2), y\n",
    "            y = y.astype(\"float32\")\n",
    "            if self.time_chunks_per_example == 1:\n",
    "                x= np.squeeze(x,axis=2)\n",
    "            if self.time_chunks_per_example > 1:\n",
    "                y = np.squeeze(y,axis=0)\n",
    "                \n",
    "            \n",
    "            yield x, y\n",
    "            \n",
    "    \n",
    "    def data_iterator(self):\n",
    "        '''\n",
    "        Args:\n",
    "           batch_size: number of examples in a batch\n",
    "           data_dir: base dir where data is\n",
    "           time_chunks_per_example: how many time steps are in a given example (default is one, but when we do 3D conv -> move to >1)\n",
    "                                - should divide evenly into 8\n",
    "        '''\n",
    "        # for each day (out of 365 days)\n",
    "        batch_size = self.batch_size\n",
    "        for tensor, masks in self._day_iterator():  #tensor is 8,16,768,1152\n",
    "            \n",
    "            tensor, min_, max_ = normalize(tensor)\n",
    "            time_chunks_per_day, variables, x,y  = tensor.shape #time_chunks will be 8\n",
    "            assert time_chunks_per_day % self.time_chunks_per_example == 0, \"For convenience, \\\n",
    "            the time chunk size should divide evenly for the number of time chunks in a single day\"\n",
    "\n",
    "            #reshapes the tensor into multiple spatiotemporal chunks of (chunk_size, 16, 768,1152)\n",
    "            spatiotemporal_tensor = tensor.reshape(time_chunks_per_day / self.time_chunks_per_example, \n",
    "                                                   self.time_chunks_per_example, variables, x ,y)\n",
    "            \n",
    "            if self.time_chunks_per_example > 1:\n",
    "                sp_mask = masks.reshape(self.time_steps_per_day / self.time_chunks_per_example, \n",
    "                                                   self.time_chunks_per_example / 2, 6 + self.num_classes, x /int(self.scale_factor) ,y / int(self.scale_factor))\n",
    "            else:\n",
    "                sp_mask = masks\n",
    "            \n",
    "            #if shuffle:\n",
    "            #    np.random.shuffle(spatiotemporal_tensor)\n",
    "\n",
    "            b = 0\n",
    "            while True:\n",
    "                if b*batch_size >= spatiotemporal_tensor.shape[0]:\n",
    "                    break\n",
    "                # todo: add labels\n",
    "\n",
    "                yield spatiotemporal_tensor[b*batch_size:(b+1)*batch_size], sp_mask[b*batch_size:(b+1)*batch_size]\n",
    "                b += 1\n",
    "                \n",
    "    def _day_iterator(self):\n",
    "        \"\"\"\n",
    "        This iterator will return a pair of  tensors:\n",
    "           * one is dimension (8, 16, 768, 1152) \n",
    "           * the other is dimension (8,12,18,9) \n",
    "                   -> 8 time steps, downsampled x, downsampled y, (xoffset, yoffset, w, h, confidence, softmax for 4 classes)\n",
    "        each tensor corresponding to one of the 365 days of the year\n",
    "        \"\"\"\n",
    "   \n",
    "        # this directory can be accessed from cori\n",
    "\n",
    "        \n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(seed=self.seed).shuffle(self.camfiles)\n",
    "        \n",
    "        for camfile in self.camfiles:\n",
    "            tr_data = self.grab_data(camfile) #one day slice per dataset\n",
    "            masks = self.make_yolo_masks_for_dataset(camfile)\n",
    "            \n",
    "            \n",
    "            if self.labels_only:\n",
    "                # we assume labels are evn time steps here\n",
    "\n",
    "                tr_data = tr_data[[0,2,4,6]]\n",
    "            #masks are always only the labels!    \n",
    "            masks = masks[[0,2,4,6]]\n",
    "            \n",
    "       \n",
    "\n",
    "            yield tr_data, masks\n",
    "            \n",
    "            \n",
    "#     def three_d_iterator(self, stride, time_steps_per_example):\n",
    "#         camfiles = get_camfiles(self.data_dir, self.years)\n",
    "#         camfiles = camfiles[:self.days]\n",
    "#         cam_ind1 = 0\n",
    "#         camfile1 = camfiles[0]\n",
    "#         camfile2 = camfiles[1]\n",
    "#         for start_step in range(0,time_steps_per_example, stride):\n",
    "#             if start_step + stride <= self.time_steps_per_day:\n",
    "                \n",
    "#                 data = self.grab_data(camfile1, time_steps=range(start_step,start_step + stride))\n",
    "#                 masks = self.make_yolo_masks_for_dataset(camfile1)\n",
    "#                 y = masks[start_step: start_step + stride]\n",
    "                \n",
    "#             else:\n",
    "#                 c1_time_steps = self.time_steps_per_day - start_step\n",
    "#                 c2_time_steps = time_steps_per_example - c1_time_steps\n",
    "#                 data1 = self.grab_data(camfile1, time_steps=range(start_step, self.time_steps_per_day))\n",
    "#                 data2 = self.grab_data(camfile2, time_steps=range(0, c2_time_steps))\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    def grab_data(self, filename, time_steps=range(8)):\n",
    "        '''takes in: filename\n",
    "           returns: (num_time_slices, num_variables,x,y) shaped tensor for day of data'''\n",
    "        if len(time_steps) == 0:\n",
    "            return\n",
    "        dataset = nc.Dataset(join(self.data_dir, filename), \"r\", format=\"NETCDF4\")\n",
    "        data = [dataset[k][time_steps] for k in self.variables]\n",
    "        tensor = np.vstack(data).reshape( len(self.variables),len(time_steps), self.xdim, self.ydim)\n",
    "        tensor = np.swapaxes(tensor,0,1)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "              \n",
    "    def make_yolo_masks_for_dataset(self, camfile_name):\n",
    "\n",
    "        labels_tensor = self.make_labels_for_dataset(camfile_name)\n",
    "        labels_tensor = convert_bbox_minmax_to_cent_xywh(labels_tensor)\n",
    "\n",
    "\n",
    "        yolo_mask = self.create_detection_gr_truth(bbox_tensor = labels_tensor)\n",
    "\n",
    "        return yolo_mask\n",
    "    \n",
    "    \n",
    "    def make_labels_for_dataset(self,fname):\n",
    "        '''takes in string for fname and the number of time_steps and outputs\n",
    "        a time_steps by maximages by 5 tensor encoding the coordinates and class of each event in a time step'''\n",
    "\n",
    "        weather_types = ['tc','etc', 'us-ar']\n",
    "        ts=get_timestamp(fname)\n",
    "        maximagespertimestep=25\n",
    "\n",
    "        # for every time step for every possible event, xmin,xmax,ymin,ymax,class\n",
    "        bboxes = np.zeros((self.time_steps_per_day, maximagespertimestep, 5))\n",
    "        event_counter = np.zeros((self.time_steps_per_day,))\n",
    "        for weather_type in weather_types:\n",
    "            selectdf = self.match_nc_to_csv(fname, weather_type)\n",
    "\n",
    "            timelist=set(selectdf[\"time_step\"])\n",
    "            for t in timelist:\n",
    "                t = int(t)\n",
    "\n",
    "                coords_for_t = selectdf[selectdf[\"time_step\"]==t].drop([\"time_step\"], axis=1).values\n",
    "                coords_for_t = coords_for_t[(coords_for_t > 0).all(1)]\n",
    "\n",
    "                # get current number of events and number of events for this time step\n",
    "                num_events_for_t = coords_for_t.shape[0]\n",
    "                cur_num_events = int(event_counter[t])\n",
    "\n",
    "                #make slice\n",
    "                slice_for_t = slice(cur_num_events, cur_num_events + num_events_for_t)\n",
    "\n",
    "                #fill variables\n",
    "                bboxes[t, slice_for_t] = coords_for_t\n",
    "                event_counter[t] += num_events_for_t\n",
    "        return bboxes\n",
    "    \n",
    "    \n",
    "    def match_nc_to_csv(self, fname, weather_type, inc_csv=False):\n",
    "        coord_keys = [\"xmin\", \"xmax\", \"ymin\", \"ymax\"]\n",
    "        ts=get_timestamp(fname)\n",
    "\n",
    "        if weather_type == 'us-ar':\n",
    "            labeldf = pd.read_csv(join(self.metadata_dir, 'ar_labels.csv'))\n",
    "            tmplabeldf=labeldf.ix[ (labeldf.month==ts.month) & (labeldf.day==ts.day) & (labeldf.year==ts.year) ].copy()\n",
    "        else:\n",
    "            labeldf = pd.read_csv(join(self.metadata_dir, '_'.join([str(ts.year),weather_type, 'labels.csv'])))\n",
    "            tmplabeldf=labeldf.ix[ (labeldf.month==ts.month) & (labeldf.day==ts.day) ].copy()\n",
    "\n",
    "\n",
    "        selectdf=tmplabeldf[[\"time_step\"]+ coord_keys + [\"category\"]]\n",
    "        if inc_csv is True:\n",
    "            return selectdf, labeldf\n",
    "        else:\n",
    "            return selectdf \n",
    "        \n",
    "        \n",
    "    \n",
    "    def create_detection_gr_truth(self,bbox_tensor):\n",
    "        #x_xy : 1,2 tuple with x and y sizes for image\n",
    "        #scale_factor: factor to scale xy size by fro gr_truth grid for YOLO\n",
    "        #scale_factor = float(scale_factor)\n",
    "        # xdim, ydim = 768,1152\n",
    "        # scale_factor = 64\n",
    "        # bbox_tensor = make_labels_for_dataset(\"cam5_1_amip_run2.cam2.h2.1984-01-03-00000.nc\")\n",
    "        # num_classes = 4 \n",
    "        num_classes = self.num_classes\n",
    "        scale_factor = float(self.scale_factor)\n",
    "        bbox_classes = bbox_tensor[:,:,4]\n",
    "        bbox_coords = bbox_tensor[:,:,:4]\n",
    "\n",
    "        #make sure xy coords divide cleanly with scale_factor\n",
    "        assert self.xdim % scale_factor == 0 and self.ydim % scale_factor == 0, \"scale factor %i must divide the xy (%i, %i) coords cleanly \" %(scale_factor,xdim, ydim)\n",
    "\n",
    "\n",
    "        x_len,y_len = self.xdim / int(scale_factor), self.ydim / int(scale_factor)\n",
    "        last_dim = 6 + num_classes #x,y,w,h,conf1,conf2 plus num_classes for one hot encoding\n",
    "\n",
    "\n",
    "        #divide up bbox with has range 0-95 to 0-95/scale_factor (so 6x6 for scale factor of 16)\n",
    "        bb_scaled = bbox_coords / scale_factor\n",
    "\n",
    "\n",
    "        #each coordinate goes at index i,j in the 6x6 array, where i,j are the coordinates of the\n",
    "        #lower left corner of the grid that center of the box (in 6x6 space ) falls on\n",
    "        #subtract eps so we dont't have one off error\n",
    "        eps = np.finfo(float).eps\n",
    "        inds = np.floor(bb_scaled[:,:,:2]-10*eps).astype('int')\n",
    "\n",
    "        #xywh where x and y are offset from lower left corner of grid thay are in [0,1] and w and h\n",
    "        # are what fraction the width and height of bboxes are of the total width and total height of the image\n",
    "        xywh = np.copy(bb_scaled)\n",
    "\n",
    "        #subtract the floored values to get the offset from the grid cell\n",
    "        xywh[:,:,:2] -= inds[:,:,:2].astype('float')\n",
    "\n",
    "\n",
    "        #divide by scaled width and height to get wdith and height relative to width and height of box\n",
    "        xywh[:,:,2] = np.log2(bbox_coords[:,:,2] / scale_factor)\n",
    "        xywh[:,:,3] = np.log2(bbox_coords[:,:,3] / scale_factor)\n",
    "\n",
    "\n",
    "        #make gr_truth which is \n",
    "\n",
    "        gr_truth = np.zeros((bbox_coords.shape[0],last_dim, x_len, y_len ))\n",
    "    #     else:\n",
    "    #         gr_truth = np.zeros((bbox_coords.shape[0], x_len,y_len,last_dim))\n",
    "\n",
    "\n",
    "        #sickens me to a do a for loop here, but numpy ain't cooperating\n",
    "        # I tried gr_truth[np.arange(gr_truth.shape[0]),inds[:0], inds[:1]][:,4] = xywh\n",
    "        #but it did not work\n",
    "\n",
    "        # we assume one box per image here\n",
    "        # for each grid point that is center of image plop in center, and width and height and class\n",
    "        for i in range(gr_truth.shape[0]):\n",
    "            #put coordinates, conf and class for all events (now there are multiple)\n",
    "            for j, coords in enumerate(xywh[i]):\n",
    "\n",
    "\n",
    "                # the index into the groudn truth grid where class should go\n",
    "                xind, yind = inds[i,j,0], inds[i,j,1]\n",
    "                gr_truth[i, :4, xind,yind,] = coords\n",
    "\n",
    "                #put in confidence\n",
    "                gr_truth[i,4,xind,yind] = 1 if bbox_classes[i,j] > 0. else 0.\n",
    "                gr_truth[i,5,xind,yind] = 1 if gr_truth[i,4,xind,yind] == 0. else 0.\n",
    "                #put in class label\n",
    "                gr_truth[i, 5 + int(bbox_classes[i,j]),xind,yind] = 1. if bbox_classes[i,j] > 0. else 0.\n",
    "\n",
    "        return gr_truth\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minw = 10000\n",
    "# minh = 10000\n",
    "# maxw= 0\n",
    "# maxh = 0\n",
    "# for weather_type in [\"etc\", \"tc\"]:\n",
    "#     for year in range(1979,2006):\n",
    "#         labeldf = pd.read_csv(join(\"/storeSSD/eracah/data/metadata\", '_'.join([str(year),weather_type, 'labels.csv'])))\n",
    "#         labeldf['widths'] = labeldf[\"xmax\"] - labeldf[\"xmin\"]\n",
    "#         labeldf['heights'] = labeldf[\"ymax\"] - labeldf[\"ymin\"]\n",
    "#         widths = labeldf['widths']\n",
    "#         heights =  labeldf['heights']\n",
    "#         if min(widths) < minw:\n",
    "#             minw = min(widths)\n",
    "            \n",
    "#         if min(heights) < minh:\n",
    "#             if min(heights) \n",
    "#             minh = min(heights)\n",
    "            \n",
    "#         if max(widths) > maxw:\n",
    "#             maxw = min(widths)\n",
    "            \n",
    "#         if max(heights) > maxh:\n",
    "#             if max(heights) == 454:\n",
    "#                 break\n",
    "#             print labeldf[['ymax', \"ymin\"]]\n",
    "#             maxh = max(heights)\n",
    "        \n",
    "\n",
    "# print maxh, maxw, minh, minw\n",
    "        \n",
    "        \n",
    "\n",
    "# f=labeldf[[\"ymin\", \"ymax\", \"heights\", \"category\", \"widths\"]]\n",
    "\n",
    "# f.ix[f[\"heights\"] == 454]\n",
    "\n",
    "# f=labeldf[[\"ymin\", \"ymax\", \"heights\"]].sort_index\n",
    "\n",
    "# f=labeldf[[\"ymin\", \"ymax\", \"heights\"]].sort\n",
    "\n",
    "# f\n",
    "\n",
    "# maxw= 0\n",
    "# maxh = 0\n",
    "# for weather_type in [\"us-ar\"]:\n",
    "#     for year in range(1979,2006):\n",
    "#         labeldf = pd.read_csv(join(\"/storeSSD/eracah/data/metadata\", 'ar_labels.csv'))\n",
    "#         widths = labeldf[\"xmax\"] - labeldf[\"xmin\"]\n",
    "#         heights = labeldf[\"ymax\"] - labeldf[\"ymin\"]\n",
    "#         if max(widths) > maxw:\n",
    "#             maxw = min(widths)\n",
    "            \n",
    "#         if max(heights) > maxh:\n",
    "#             maxh = max(heights)\n",
    "\n",
    "# maxw\n",
    "\n",
    "# maxh\n",
    "\n",
    "# !ls /storeSSD/eracah/data/metadata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def count_events(year):\n",
    "    metadata_dir = \"/storeSSD/eracah/data/metadata/\"\n",
    "    coord_keys = [\"xmin\", \"xmax\", \"ymin\", \"ymax\"]\n",
    "    d ={\"us-ar\":0, \"etc\":0, \"tc\":0}\n",
    "    for weather_type in d.keys(): \n",
    "        if weather_type == 'us-ar':\n",
    "            labeldf = pd.read_csv(join(metadata_dir, 'ar_labels.csv'))\n",
    "            labeldf = labeldf.ix[(labeldf.year==year)]\n",
    "            d[weather_type] = len(labeldf)\n",
    "#             tmplabeldf=labeldf.ix[ (labeldf.month==ts.month) & (labeldf.day==ts.day) & (labeldf.year==ts.year) ].copy()\n",
    "        else:\n",
    "            labeldf = pd.read_csv(join(metadata_dir, '_'.join([str(year),weather_type, 'labels.csv'])))\n",
    "#             tmplabeldf=labeldf.ix[ (labeldf.month==ts.month) & (labeldf.day==ts.day) ].copy()\n",
    "            if weather_type == \"etc\":\n",
    "                d[weather_type] = len(labeldf)\n",
    "            else:\n",
    "                d[\"td\"] = len(labeldf[labeldf[\"str_category\"] == \"tropical_depression\"])\n",
    "                d[\"tc\"] = len(labeldf[labeldf[\"str_category\"] == \"tropical_cyclone\"])\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_events_md(year, last_md = (3,16)):\n",
    "    lm, ld = last_md\n",
    "    metadata_dir = \"/storeSSD/eracah/data/metadata/\"\n",
    "    coord_keys = [\"xmin\", \"xmax\", \"ymin\", \"ymax\"]\n",
    "    d ={\"us-ar\":0, \"etc\":0, \"tc\":0}\n",
    "    for weather_type in d.keys(): \n",
    "        if weather_type == 'us-ar':\n",
    "            labeldf = pd.read_csv(join(metadata_dir, 'ar_labels.csv'))\n",
    "            for i in range(1,lm):\n",
    "                labeldf = labeldf.ix[(labeldf.year==year) & (labeldf.month == i)]\n",
    "                d[weather_type] += len(labeldf)\n",
    "            for i in range(1,ld+1):\n",
    "                labeldf_end = labeldf.ix[(labeldf.year==year) & (labeldf.month ==lm) & (labeldf.day == i)]\n",
    "                d[weather_type] += len(labeldf_end)\n",
    "           \n",
    "#             tmplabeldf=labeldf.ix[ (labeldf.month==ts.month) & (labeldf.day==ts.day) & (labeldf.year==ts.year) ].copy()\n",
    "        else:\n",
    "            labeldf = pd.read_csv(join(metadata_dir, '_'.join([str(year),weather_type, 'labels.csv'])))\n",
    "            if weather_type == \"etc\":\n",
    "                for i in range(1,lm):\n",
    "                    labeldfi = labeldf.ix[(labeldf.year==year) & (labeldf.month == i)]\n",
    "                    d[weather_type] += len(labeldfi)\n",
    "                for i in range(1,ld+1):\n",
    "                    labeldfi = labeldf.ix[(labeldf.year==year) & (labeldf.month ==lm) & (labeldf.day == i)]\n",
    "                    d[weather_type] += len(labeldfi)\n",
    "            else:\n",
    "                d[\"td\"] = 0\n",
    "                for i in range(1,lm):\n",
    "                    labeldfi = labeldf.ix[(labeldf.year==year) & (labeldf.month == i)]\n",
    "                    d[\"td\"] += len(labeldfi[labeldfi[\"str_category\"] == \"tropical_depression\"])\n",
    "                    d[\"tc\"] += len(labeldfi[labeldfi[\"str_category\"] == \"tropical_cyclone\"])\n",
    "                for i in range(1,ld+1):\n",
    "                    labeldfi = labeldf.ix[(labeldf.year==year) & (labeldf.month ==lm) & (labeldf.day == i)]\n",
    "                    d[\"td\"] += len(labeldfi[labeldfi[\"str_category\"] == \"tropical_depression\"])\n",
    "                    d[\"tc\"] += len(labeldfi[labeldfi[\"str_category\"] == \"tropical_cyclone\"])\n",
    "              \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_percents(event_dict):\n",
    "    tot = sum(event_dict.values())\n",
    "    new_d = {k: float(v)/ tot for k,v in event_dict.iteritems()}\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'etc': 0.4657025341647871,\n",
       " 'tc': 0.42324532307284063,\n",
       " 'td': 0.05744991375878997,\n",
       " 'us-ar': 0.05360222900358233}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_percents(count_events(1979))\n",
    "\n",
    "# count_events(1984)\n",
    "\n",
    "# count_events_md(1982)\n",
    "\n",
    "# count_events_md(1985)\n",
    "\n",
    "# get_percents(count_events(1984))\n",
    "\n",
    "# 365 * 8\n",
    "\n",
    "# metadata_dir = \"/storeSSD/eracah/data/metadata/\"\n",
    "\n",
    "# labeldf = pd.read_csv(join(metadata_dir, '_'.join([str(1979),\"tc\", 'labels.csv'])))\n",
    "\n",
    "# len(labeldf[labeldf[\"str_category\"] == \"tropical_depression\"])\n",
    "\n",
    "# len(labeldf[labeldf[\"str_category\"] == \"tropical_cyclone\"])\n",
    "\n",
    "# range(1,3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
